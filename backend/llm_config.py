"""
LLM Configuration
Centralized configuration for LLM models used in the application.
Easy switching between different models for testing and production.
"""

# ============================================================================
# LLM MODEL CONFIGURATION
# ============================================================================

# Current Model Selection (uncomment one)
# ----------------------------------------

# ğŸš€ OPTION 1: Lightweight Model (Recommended for Testing)
# ~250MB download, fast inference, good for development
# Works well on CPU, minimal memory requirements
# CURRENT_LLM_MODEL = "google/flan-t5-base"

# ğŸš€ OPTION 2: Better Quality (Medium)
# ~1GB download, better quality answers
# CURRENT_LLM_MODEL = "google/flan-t5-large"

# ğŸš€ OPTION 3: Best Quality (Large) 
# ~3GB download, highest quality
# CURRENT_LLM_MODEL = "google/flan-t5-xl"

# ğŸš€ OPTION 4: Production Model (Uzbek Language) âœ… ACTIVE
# ~16GB download, requires GPU with 8GB+ VRAM
# Best for Uzbek language understanding
CURRENT_LLM_MODEL = "behbudiy/Llama-3.1-8B-Instruct-Uz"

# ğŸš€ OPTION 5: General Purpose (Good multilingual support)
# ~13GB download, requires GPU
# CURRENT_LLM_MODEL = "meta-llama/Llama-2-7b-chat-hf"


# ============================================================================
# EMBEDDING MODEL CONFIGURATION
# ============================================================================

# Embedding model for RAG (text similarity)
# ~420MB download, works well for multilingual texts including Uzbek
EMBEDDING_MODEL = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"

# Alternative embedding models:
# EMBEDDING_MODEL = "sentence-transformers/all-MiniLM-L6-v2"  # Lighter, English-focused
# EMBEDDING_MODEL = "intfloat/multilingual-e5-small"  # Good multilingual support


# ============================================================================
# GENERATION PARAMETERS
# ============================================================================

# Device configuration
# "auto" - automatically select GPU if available, else CPU
# "cuda" - force GPU usage (will fail if no GPU)
# "cpu" - force CPU usage
DEVICE = "auto"

# Text generation parameters
MAX_NEW_TOKENS = 256  # Maximum length of generated answer
TEMPERATURE = 0.7     # Higher = more creative, Lower = more focused
TOP_P = 0.9          # Nucleus sampling parameter
TOP_K = 50           # Top-K sampling parameter

# RAG parameters
K_DOCUMENTS = 3      # Number of context documents to retrieve
CHUNK_SIZE = 1000    # Size of text chunks for processing
CHUNK_OVERLAP = 200  # Overlap between chunks


# ============================================================================
# MODEL COMPARISON
# ============================================================================

MODEL_COMPARISON = """
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ MODEL COMPARISON GUIDE                                                  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚ 1. google/flan-t5-base (RECOMMENDED FOR TESTING) âœ…                     â”‚
â”‚    Size: ~250MB                                                         â”‚
â”‚    Speed: âš¡âš¡âš¡âš¡âš¡ (Very Fast)                                         â”‚
â”‚    Quality: â­â­â­ (Good)                                              â”‚
â”‚    Memory: ~1GB RAM                                                     â”‚
â”‚    Best for: Development, testing, CPU-only machines                   â”‚
â”‚                                                                         â”‚
â”‚ 2. google/flan-t5-large                                                â”‚
â”‚    Size: ~1GB                                                           â”‚
â”‚    Speed: âš¡âš¡âš¡âš¡ (Fast)                                                â”‚
â”‚    Quality: â­â­â­â­ (Very Good)                                        â”‚
â”‚    Memory: ~2GB RAM                                                     â”‚
â”‚    Best for: Better quality on decent hardware                         â”‚
â”‚                                                                         â”‚
â”‚ 3. google/flan-t5-xl                                                   â”‚
â”‚    Size: ~3GB                                                           â”‚
â”‚    Speed: âš¡âš¡âš¡ (Medium)                                                â”‚
â”‚    Quality: â­â­â­â­â­ (Excellent)                                      â”‚
â”‚    Memory: ~4GB RAM                                                     â”‚
â”‚    Best for: Production with good CPU or basic GPU                     â”‚
â”‚                                                                         â”‚
â”‚ 4. behbudiy/Llama-3.1-8B-Instruct-Uz (UZBEK OPTIMIZED)                â”‚
â”‚    Size: ~16GB                                                          â”‚
â”‚    Speed: âš¡âš¡ (Slow on CPU, Fast on GPU)                              â”‚
â”‚    Quality: â­â­â­â­â­ (Excellent for Uzbek)                           â”‚
â”‚    Memory: ~8GB VRAM (GPU) or 16GB+ RAM (CPU)                          â”‚
â”‚    Best for: Production with Uzbek language focus                      â”‚
â”‚                                                                         â”‚
â”‚ 5. meta-llama/Llama-2-7b-chat-hf                                       â”‚
â”‚    Size: ~13GB                                                          â”‚
â”‚    Speed: âš¡âš¡ (Requires GPU)                                           â”‚
â”‚    Quality: â­â­â­â­â­ (Excellent general purpose)                      â”‚
â”‚    Memory: ~7GB VRAM                                                    â”‚
â”‚    Best for: Production with GPU, multilingual support                 â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

RECOMMENDATION:
  ğŸ‘‰ For Testing/Development: Use flan-t5-base
  ğŸ‘‰ For Production (Uzbek): Use Llama-3.1-8B-Instruct-Uz with GPU âœ… ACTIVE
  ğŸ‘‰ For Production (General): Use flan-t5-xl or Llama-2-7b-chat
"""


# ============================================================================
# HELPER FUNCTIONS
# ============================================================================

def get_llm_config():
    """Get current LLM configuration."""
    return {
        "model_name": CURRENT_LLM_MODEL,
        "embedding_model": EMBEDDING_MODEL,
        "device": DEVICE,
        "max_new_tokens": MAX_NEW_TOKENS,
        "temperature": TEMPERATURE,
        "k_documents": K_DOCUMENTS
    }


def print_config():
    """Print current configuration."""
    print("=" * 70)
    print("ğŸ“Š CURRENT LLM CONFIGURATION")
    print("=" * 70)
    print(f"LLM Model:       {CURRENT_LLM_MODEL}")
    print(f"Embedding Model: {EMBEDDING_MODEL}")
    print(f"Device:          {DEVICE}")
    print(f"Max Tokens:      {MAX_NEW_TOKENS}")
    print(f"Temperature:     {TEMPERATURE}")
    print(f"K Documents:     {K_DOCUMENTS}")
    print("=" * 70)
    print(MODEL_COMPARISON)


if __name__ == "__main__":
    print_config()
